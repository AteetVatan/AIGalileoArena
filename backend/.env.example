# -----------------------------------------------
# Galileo Arena -- Backend Environment Variables
# Copy to .env and fill in your values:
#   cp .env.example .env
# -----------------------------------------------

# -----------------------------------------------
# Database Configuration
# -----------------------------------------------
# Local development:
# DATABASE_URL=postgresql+asyncpg://galileo:galileo_pass@localhost:5432/galileo_arena
# Supabase (replace <project-ref>, <password>, <region>):
DATABASE_URL=postgresql+asyncpg://app_user.<project-ref>:<password>@aws-0-<region>.pooler.supabase.com:5432/postgres
# Migrations (postgres role, Session pooler):
DATABASE_URL_MIGRATIONS=postgresql+asyncpg://postgres.<project-ref>:<password>@aws-0-<region>.pooler.supabase.com:5432/postgres

# -----------------------------------------------
# LLM Provider API Keys
# At least one API key is required to run evaluations.
# -----------------------------------------------

# OpenAI — Models: gpt-4o, gpt-4o-mini
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic — Models: claude-sonnet-4-20250514
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Mistral — Models: mistral-large-latest
MISTRAL_API_KEY=your_mistral_api_key_here

# DeepSeek — Models: deepseek-chat, deepseek-reasoner
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Google Gemini — Models: gemini-2.0-flash, gemini-1.5-pro
GEMINI_API_KEY=your_gemini_api_key_here

# xAI Grok — Models: grok-3, grok-3-mini
GROK_API_KEY=your_grok_api_key_here

# -----------------------------------------------
# Logging
# -----------------------------------------------
LOG_LEVEL=INFO

# -----------------------------------------------
# ML Scoring (ONNX)
# Set to false to use keyword-only scoring (no ONNX models needed).
# See SETUP.md §8 for full ML configuration reference.
# -----------------------------------------------
ML_SCORING_ENABLED=true
# ML_MODELS_DIR=models
# ONNX_INTRA_THREADS=2
# ML_MAX_WORKERS=2
# ML_NLI_MAX_TOKENS=384
# ML_FALSIFIABLE_THRESHOLD=0.45
# ML_DEFERENCE_THRESHOLD_LOW=0.4
# ML_DEFERENCE_THRESHOLD_MID=0.6
# ML_DEFERENCE_THRESHOLD_HIGH=0.8
# ML_REFUSAL_THRESHOLD=0.6

# -----------------------------------------------
# Galileo Analytics — Freshness Sweep
# -----------------------------------------------
# Enable automated sweep scheduler (daily cron)
SWEEP_ENABLED=false
# UTC hour for daily sweep (0-23)
SWEEP_CRON_HOUR=3
# Cases per model per sweep
SWEEP_CASES_COUNT=5
# Budget guardrails
SWEEP_MAX_EVALS_PER_RUN=50
SWEEP_MAX_PARALLEL=3
SWEEP_MAX_COST_USD=5.0
# Also run baseline mode during sweep
SWEEP_INCLUDE_BASELINE=true

# -----------------------------------------------
# Analytics Query Timeouts (seconds)
# -----------------------------------------------
# ANALYTICS_TIMEOUT_SUMMARY_S=5
# ANALYTICS_TIMEOUT_TREND_S=5
# ANALYTICS_TIMEOUT_HEATMAP_S=15
# ANALYTICS_TIMEOUT_DISTRIBUTION_S=10
# ANALYTICS_TIMEOUT_DEFAULT_S=10
# ANALYTICS_CACHE_TTL_S=60

# -----------------------------------------------
# Environment Mode
# -----------------------------------------------
# DEBUG=true  → all models unlocked, no daily caps, scheduler unrestricted
# DEBUG=false → production: model allowlist + daily caps enforced
DEBUG=true

# -----------------------------------------------
# Debate Mode — Production Restrictions
# Only enforced when DEBUG=false. Ignored in debug mode.
# -----------------------------------------------
# Max debate calls per model per calendar day (resets at midnight in APP_TIMEZONE)
DEBATE_DAILY_CAP=3
# Comma-separated provider/model_name pairs allowed in production debate mode
DEBATE_ENABLED_MODELS=mistral/mistral-large-latest,deepseek/deepseek-chat
# Timezone for daily cap resets and scheduler triggers (IANA name)
APP_TIMEZONE=Europe/Berlin

# -----------------------------------------------
# Monthly Eval Scheduler — Production Only
# Only activates when DEBUG=false AND EVAL_SCHEDULER_ENABLED=true.
# -----------------------------------------------
EVAL_SCHEDULER_ENABLED=false
# Day of month to run (1-28)
EVAL_SCHEDULER_CRON_DAY=1
# Hour in APP_TIMEZONE to run (0-23)
EVAL_SCHEDULER_CRON_HOUR=2
# Number of random datasets to evaluate per run
EVAL_SCHEDULER_DATASETS=6
# Random cases per dataset per run
EVAL_SCHEDULER_CASES=1

# -----------------------------------------------
# Concurrency Tuning
# -----------------------------------------------
# DB connection pool (per uvicorn worker)
# DB_POOL_SIZE=10
# DB_MAX_OVERFLOW=15
# DB_POOL_TIMEOUT=30
# Max concurrent background LLM evaluation tasks
# MAX_CONCURRENT_RUNS=20
# Uvicorn worker processes (production only; debug.py always uses 1)
# UVICORN_WORKERS=4